import torch
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from peft import PeftModel
import os

# ================= ì„¤ì • =================
# GPU ì„¤ì • (í•„ìš”ì‹œ ë³€ê²½)
os.environ["CUDA_VISIBLE_DEVICES"] = "3"

BASE_ID = "Qwen/Qwen2.5-VL-3B-Instruct"
# ë°©ê¸ˆ í•™ìŠµí•œ ì–´ëŒ‘í„° ê²½ë¡œ
ADAPTER_PATH = "./checkpoints/mvsm_baseline_paper"
# í•©ì³ì„œ ì €ì¥í•  ê²½ë¡œ
SAVE_PATH = "./checkpoints/mvsm_baseline_merged"
# ========================================

def merge():
    print(f"ğŸ”„ Merging: {ADAPTER_PATH} -> {SAVE_PATH}")
    
    # Base Model ë¡œë“œ
    base = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        BASE_ID, torch_dtype=torch.bfloat16, device_map="auto"
    )
    
    # LoRA í•©ì¹˜ê¸°
    model = PeftModel.from_pretrained(base, ADAPTER_PATH)
    model = model.merge_and_unload()
    
    # ì €ì¥
    model.save_pretrained(SAVE_PATH)
    
    # Processor ì €ì¥
    try:
        processor = AutoProcessor.from_pretrained(BASE_ID, min_pixels=256*28*28, max_pixels=1280*28*28)
    except:
        processor = AutoProcessor.from_pretrained(BASE_ID)
    processor.save_pretrained(SAVE_PATH)
    
    print(f"âœ¨ Merge ì™„ë£Œ! ì €ì¥ëœ ê²½ë¡œ: {SAVE_PATH}")

if __name__ == "__main__":
    merge()