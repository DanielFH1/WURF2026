import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from peft import PeftModel
import os

# ================= ì„¤ì • =================
# 1. í•™ìŠµëœ Real CoT LoRA ê²½ë¡œ
ADAPTER_DIR = "./checkpoints/mvsm_real_cot_v1"

# 2. ë² ì´ìŠ¤ ëª¨ë¸ ID (â˜… ì¤‘ìš”: í•™ìŠµí•  ë•Œ ì¼ë˜ 2.5 ë²„ì „ì´ì–´ì•¼ í•¨)
BASE_MODEL_ID = "Qwen/Qwen2.5-VL-3B-Instruct"

# 3. ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
OUTPUT_DIR = "./checkpoints/mvsm_real_cot_merged"
# ========================================

def merge():
    print(f"ğŸš€ Loading Base Model: {BASE_MODEL_ID} (CPU Mode)...")
    # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ CPUë¡œ ë¡œë“œ
    base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        BASE_MODEL_ID,
        torch_dtype=torch.float16,
        device_map="cpu",
    )

    print(f"ğŸš€ Loading Real CoT LoRA Adapter from: {ADAPTER_DIR}")
    model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)
    
    print("ğŸ”„ Merging LoRA into Base Model...")
    model = model.merge_and_unload()
    
    print(f"ğŸ’¾ Saving merged model to: {OUTPUT_DIR}")
    model.save_pretrained(OUTPUT_DIR)
    
    print("ğŸ’¾ Saving processor...")
    processor = AutoProcessor.from_pretrained(BASE_MODEL_ID)
    processor.save_pretrained(OUTPUT_DIR)
    
    print("âœ¨ Merge Complete! ì´ì œ í‰ê°€(Evaluate)ë¥¼ ì§„í–‰í•˜ì„¸ìš”.")

if __name__ == "__main__":
    merge()