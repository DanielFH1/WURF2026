# ==========================================
import os
# [OOM ë°©ì§€] ë©”ëª¨ë¦¬ íŒŒí¸í™” ë°©ì§€ ì„¤ì •
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# ==========================================

import json
import torch
import wandb
from transformers import (
    Qwen2_5_VLForConditionalGeneration,
    AutoProcessor,
    Trainer,
    TrainingArguments
)
from qwen_vl_utils import process_vision_info
from peft import LoraConfig, get_peft_model, TaskType

# ================= ì„¤ì • (Augmentation ì ìš©) =================
# W&B í”„ë¡œì íŠ¸ ì´ë¦„
os.environ["WANDB_PROJECT"] = "ViewSpatial-DeepDive"

MODEL_ID = "Qwen/Qwen2.5-VL-3B-Instruct"

# [ë³€ê²½ 1] ê²°ê³¼ê°€ ì €ì¥ë  í´ë” ì´ë¦„ ë³€ê²½ (Baselineê³¼ ì„ì´ì§€ ì•Šê²Œ)
OUTPUT_DIR = "./checkpoints/mvsm_aug_flip_v1"

# [ë³€ê²½ 2] ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_DIR = "data_train_scene_split"
# â˜… ì¦ê°•ëœ ë°ì´í„° íŒŒì¼ ì‚¬ìš©
TRAIN_FILE = os.path.join(DATA_DIR, "train_augmented.jsonl") 
VAL_FILE = os.path.join(DATA_DIR, "val.jsonl")

# [ë³€ê²½ 3] ì´ë¯¸ì§€ ë£¨íŠ¸ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ)
# ì´ ê²½ë¡œë¥¼ ì´ë¯¸ì§€ íŒŒì¼ëª… ì•ì— ë¶™ì—¬ì„œ ë¡œë”ê°€ íŒŒì¼ì„ ëª» ì°¾ëŠ” ë¬¸ì œ í•´ê²°
IMAGE_ROOT = "/nas_data2/seungwoo/2/ViewSpatial-Bench"

# ======================================================

def load_dataset(file_path):
    data = []
    print(f"ğŸ“‚ Loading: {file_path}")
    with open(file_path, 'r') as f:
        for line in f:
            item = json.loads(line)
            
            # â˜… ì´ë¯¸ì§€ ê²½ë¡œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (File Not Found ë°©ì§€)
            if 'messages' in item:
                for msg in item['messages']:
                    if msg['role'] == 'user':
                        for content in msg['content']:
                            if content['type'] == 'image':
                                # "augmented_images/..." ë“±ì„ "/nas.../augmented_images/..."ë¡œ ë³€í™˜
                                content['image'] = os.path.join(IMAGE_ROOT, content['image'])
            
            data.append(item)
    return data

class QwenDataCollator:
    def __init__(self, processor):
        self.processor = processor
    def __call__(self, batch):
        texts, images = [], []
        for item in batch:
            messages = item['messages']
            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
            image_inputs, _ = process_vision_info(messages)
            texts.append(text)
            images.append(image_inputs)
        
        # Processorê°€ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ê³  í…ì„œë¡œ ë³€í™˜
        inputs = self.processor(text=texts, images=images, padding=True, return_tensors="pt")
        
        labels = inputs["input_ids"].clone()
        labels[labels == self.processor.tokenizer.pad_token_id] = -100
        image_token_id = self.processor.tokenizer.convert_tokens_to_ids("<|image_pad|>")
        labels[labels == image_token_id] = -100
        inputs["labels"] = labels
        return inputs

def train():
    # [ë³€ê²½ 4] WandB Run ì´ë¦„ ë³€ê²½
    wandb.init(name="Augment-Flip-Rank16-Epoch3")
    
    # 1. ëª¨ë¸ ë¡œë“œ
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        MODEL_ID, torch_dtype=torch.bfloat16, device_map="auto"
    )
    processor = AutoProcessor.from_pretrained(MODEL_ID, min_pixels=256*28*28, max_pixels=1280*28*28)

    model.enable_input_require_grads()

    # 2. LoRA ì„¤ì •
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=16,               
        lora_alpha=32,      
        lora_dropout=0.1,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # 3. ë°ì´í„° ì¤€ë¹„
    train_dataset = load_dataset(TRAIN_FILE)
    val_dataset = load_dataset(VAL_FILE)

    # 4. í•™ìŠµ ì¸ì
    args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=1, 
        gradient_accumulation_steps=16,
        gradient_checkpointing=True,
        num_train_epochs=3,         
        learning_rate=2e-5,         
        logging_steps=10,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=1,
        bf16=True,
        report_to="wandb",
        dataloader_num_workers=4,
        remove_unused_columns=False
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=QwenDataCollator(processor)
    )

    model.config.use_cache = False 

    trainer.train()
    trainer.save_model(OUTPUT_DIR)
    processor.save_pretrained(OUTPUT_DIR)
    print(f"âœ¨ Augmentation í•™ìŠµ ì™„ë£Œ: {OUTPUT_DIR}")

if __name__ == "__main__":
    train()