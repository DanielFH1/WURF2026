import os, re, csv, json, torch, base64
import random, argparse
import numpy as np
from PIL import Image
from random import seed
from tqdm.auto import tqdm
from collections import defaultdict
from transformers import AutoProcessor
from transformers import Qwen2_5_VLForConditionalGeneration
from qwen_vl_utils import process_vision_info

seed(1234)
np.random.seed(1234)

parser = argparse.ArgumentParser()
parser.add_argument("--model_path", type=str, required=True)
parser.add_argument("--dataset_path", type=str, default="data_train_scene_split/test.json")
parser.add_argument("--image_folder", type=str, default="/nas_data2/seungwoo/2/ViewSpatial-Bench")
args = parser.parse_args()

model_path = args.model_path
model_name = model_path.split("/")[-1] or model_path.split("/")[-2]
dataset_path = args.dataset_path
image_root = args.image_folder

# [ìˆ˜ì •] CoT ëª¨ë¸ì€ Chat Templateì´ ìžë™ìœ¼ë¡œ Assistant í„´ì„ ë„˜ê²¨ì£¼ë¯€ë¡œ, 
# ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸("Answer:") ì¶”ê°€ëŠ” ì œê±°í•˜ì—¬ í•™ìŠµ ë•Œì™€ ì¡°ê±´ì„ ë§žì¶¥ë‹ˆë‹¤.
prompt_format = "" 

print(f"ðŸš€ Loading CoT Model from: {model_path}")
# [í™•ì¸] Qwen2.5 í´ëž˜ìŠ¤ ì‚¬ìš© OK
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    model_path, torch_dtype=torch.bfloat16, device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_path, min_pixels=256*28*28, max_pixels=1280*28*28)

def extract_option_cot(text):
    """
    CoT ëª¨ë¸ì˜ ê¸´ ë‹µë³€ì—ì„œ ì§„ì§œ ì •ë‹µì„ ì°¾ì•„ë‚´ëŠ” ë˜‘ë˜‘í•œ íŒŒì‹± í•¨ìˆ˜
    """
    if not text: return None
    
    # 1. ëª…í™•í•œ ê²°ë¡  íŒ¨í„´ ê²€ìƒ‰ ("Therefore, the correct option is A")
    patterns = [
        r"correct option is ([A-D])",
        r"answer is ([A-D])",
        r"Option ([A-D])",
        r"Therefore, ([A-D])",
        r"Therefore, the answer is ([A-D])"
    ]
    for p in patterns:
        match = re.search(p, text, re.IGNORECASE)
        if match: return match.group(1).upper()

    # 2. íŒ¨í„´ì´ ì—†ìœ¼ë©´, ë¬¸ìž¥ì˜ ë§ˆì§€ë§‰ì— ë“±ìž¥í•˜ëŠ” A-Dë¥¼ ì •ë‹µìœ¼ë¡œ ê°„ì£¼
    matches = re.findall(r"\b([A-D])\b", text)
    if matches:
        return matches[-1].upper()
        
    return None

def url_to_base64(url):
    full_path = os.path.join(image_root, url)
    if not os.path.exists(full_path):
        if os.path.exists(url): full_path = url 
        
    if os.path.exists(full_path):
        with open(full_path, "rb") as f:
            return "data:image/jpeg;base64," + base64.b64encode(f.read()).decode("utf-8")
    return False

def get_output(image_paths, question):
    if isinstance(image_paths, str): image_paths = [image_paths]
    
    image_url = [url_to_base64(img) for img in image_paths]
    image_url = [img for img in image_url if img is not False]
    
    # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì²˜ë¦¬
    if not image_url: return "C"

    content = [{"type": "image", "image": path, "resized_height": 280, "resized_width": 420} for path in image_url]
    
    messages = [{
        "role": "user",
        "content": [*content, {"type": "text", "text": question}]
    }]
    
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    
    inputs = processor(
        text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt"
    ).to("cuda")
    
    # [í™•ì¸] CoT ìƒì„±ì„ ìœ„í•´ ì¶©ë¶„í•œ í† í° ìˆ˜ (512) í™•ë³´ OK
    generated_ids = model.generate(**inputs, max_new_tokens=512)
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]
    
    return output_text

def evaluate_vlm():
    print(f"Evaluating on: {dataset_path}")
    with open(dataset_path, "r", encoding="utf-8") as f:
        benchmark_data = json.load(f)

    stats = defaultdict(lambda: {"correct": 0, "total": 0})
    total_correct = 0
    total_questions = 0

    output_path = f"result/{model_name}"
    os.makedirs(output_path, exist_ok=True)
    result_file = f"{output_path}/result_real_cot.csv"
    
    with open(result_file, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["ID", "Question_Type", "Full_Output", "Parsed_Answer", "Correct_Answer", "IsCorrect"])

        for i, item in enumerate(tqdm(benchmark_data)):
            try:
                image_path = item['image_path']
                # prompt_formatì€ ì œê±°í–ˆìœ¼ë¯€ë¡œ ìˆœìˆ˜ ì§ˆë¬¸ë§Œ ë“¤ì–´ê°
                question = item["question"] + item.get("choices", "") + prompt_format
                correct_answer = item["answer"]
                question_type = item["question_type"]
                
                full_output = get_output(image_path, question)
                parsed_pred = extract_option_cot(full_output)
                
                # ì •ë‹µ ë¹„êµ (ë¬¸ìžì—´ ë¹„êµ)
                # ì •ë‹µ(correct_answer)ë„ "A. Left" í˜•íƒœì¼ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ íŒŒì‹± í•„ìš”
                parsed_gt = extract_option_cot(correct_answer)
                if not parsed_gt: # ì •ë‹µì´ ê·¸ëƒ¥ "A"ì¸ ê²½ìš°
                    parsed_gt = correct_answer.strip().upper()[0]

                is_correct = (parsed_pred == parsed_gt) if parsed_pred else False
                
                stats[question_type]["total"] += 1
                total_questions += 1
                if is_correct:
                    stats[question_type]["correct"] += 1
                    total_correct += 1
                    
                writer.writerow([i, question_type, full_output, parsed_pred, correct_answer, is_correct])
                
            except Exception as e:
                print(f"Error on item {i}: {e}")
                continue

    print("\nBenchmark Evaluation Results (Real CoT):")
    print("-" * 60)
    for qtype, values in stats.items():
        if values["total"] > 0:
            print(f"{qtype}: {values['correct']}/{values['total']} = {values['correct']/values['total']:.2%}")
    print("-" * 60)
    if total_questions > 0:
        print(f"Total Accuracy: {total_correct/total_questions:.2%} ({total_correct}/{total_questions})")
    print(f"Result saved to {result_file}")

if __name__ == '__main__':
    evaluate_vlm()