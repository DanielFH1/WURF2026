import matplotlib.pyplot as plt
import numpy as np

# ========================================================
# ğŸ“Š ì‹¤í—˜ 3ê°œ ë¹„êµ ë°ì´í„°
# ========================================================

# 1. ì¹´í…Œê³ ë¦¬
categories = [
    "Camera\nObj View",       
    "Camera\nRel Dir",       
    "Person\nObj View",       
    "Person\nRel Dir",       
    "Person\nScene Sim",      
    "TOTAL\nACCURACY"         
]

# 2. ë°ì´í„° ì…ë ¥ (Hardcoded)
# (1) Baseline (Vanilla)
baseline_scores = [70.70, 59.40, 89.10, 59.30, 67.00, 67.69]

# (2) Visual Prompt (Set-of-Mark)
# - íŠ¹ì§•: Grounding(ìœ„ì¹˜ ì°¾ê¸°) í•„ìš”í•œ Taskì—ì„œ ê°•ì„¸, ì „ì²´ ë§¥ë½ì—ì„œ ì•½ì„¸
visual_scores =   [72.83, 58.89, 86.96, 61.73, 56.88, 65.88]

# (3) CoT (Chain-of-Thought)
# - íŠ¹ì§•: ì‹œê° ì •ë³´ ì—†ì´ ë§ë§Œ ê¸¸ì–´ì ¸ì„œ ì „ì²´ì ìœ¼ë¡œ ì„±ëŠ¥ í•˜ë½ (Hallucination)
cot_scores =      [60.87, 55.00, 77.17, 48.15, 45.87, 56.86]

# ========================================================

def plot_triple_benchmark():
    # ê·¸ë˜í”„ ì„¤ì •
    x = np.arange(len(categories))
    width = 0.25  # ë§‰ëŒ€ í­ ì¡°ì ˆ

    # 1. í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
    print("\n" + "="*80)
    print(f"ğŸ“Š Triple Comparison: Baseline vs Visual Prompt vs CoT")
    print("="*80)
    print(f"{'Category':<25} | {'Base':<7} | {'Visual':<7} | {'CoT':<7} | {'Best Method'}")
    print("-" * 80)
    
    for i, cat in enumerate(categories):
        cat_name = cat.replace('\n', ' ')
        b = baseline_scores[i]
        v = visual_scores[i]
        c = cot_scores[i]
        
        # ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
        scores = {'Base': b, 'Visual': v, 'CoT': c}
        best_method = max(scores, key=scores.get)
        best_score = scores[best_method]
        
        # Best Method í‘œì‹œëŠ” ìƒ‰ê¹” ëŒ€ì‹  í…ìŠ¤íŠ¸ë¡œ
        print(f"{cat_name:<25} | {b:<7.2f} | {v:<7.2f} | {c:<7.2f} | {best_method} ({best_score:.2f}%)")
        
    print("="*80)

    # 2. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    fig, ax = plt.subplots(figsize=(15, 8))
    
    # ìƒ‰ìƒ í…Œë§ˆ
    c_base = '#A9A9A9'   # íšŒìƒ‰ (Baseline)
    c_vis = '#d62728'    # ë¹¨ê°• (Visual Prompt - ê°•ë ¬í•¨)
    c_cot = '#1f77b4'    # íŒŒë‘ (CoT - ì°¨ë¶„í•¨/ë…¼ë¦¬)

    # ë§‰ëŒ€ ìƒì„± (ìœ„ì¹˜ ì¡°ì •: x-width, x, x+width)
    rects1 = ax.bar(x - width, baseline_scores, width, label='Baseline', color=c_base, alpha=0.7)
    rects2 = ax.bar(x, visual_scores, width, label='Visual Prompt', color=c_vis, alpha=0.9)
    rects3 = ax.bar(x + width, cot_scores, width, label='Chain-of-Thought', color=c_cot, alpha=0.7)

    # ê·¸ë˜í”„ ê¾¸ë¯¸ê¸°
    ax.set_ylabel('Accuracy (%)', fontsize=12)
    ax.set_title('Impact of Spatial Strategies on VLM Performance', fontsize=15, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(categories, fontsize=11, fontweight='bold')
    ax.set_ylim(0, 100)
    ax.legend(fontsize=12, loc='upper right')
    ax.grid(axis='y', linestyle='--', alpha=0.3)

    # ê°’ í‘œì‹œ í•¨ìˆ˜
    def autolabel(rects):
        for rect in rects:
            height = rect.get_height()
            ax.annotate(f'{height:.1f}',
                        xy=(rect.get_x() + rect.get_width() / 2, height),
                        xytext=(0, 3),  # 3 points vertical offset
                        textcoords="offset points",
                        ha='center', va='bottom', fontsize=9, fontweight='bold')

    autolabel(rects1)
    autolabel(rects2)
    autolabel(rects3)

    # Insight ë°•ìŠ¤ ì¶”ê°€
    textstr = '\n'.join((
        r'$\bf{Analysis}$:',
        r'- Base: Best overall stability',
        r'- Visual: Strong in "Relative Direction"',
        r'- CoT: Degraded due to hallucination',
    ))
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.3)
    ax.text(0.02, 0.95, textstr, transform=ax.transAxes, fontsize=12,
            verticalalignment='top', bbox=props)

    plt.tight_layout()
    output_path = "triple_comparison.svg"
    plt.savefig(output_path)
    print(f"\nğŸ–¼ï¸  Graph saved to: {output_path}")

if __name__ == "__main__":
    plot_triple_benchmark()