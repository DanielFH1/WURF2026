import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from peft import PeftModel
import os

# ================= ì„¤ì • =================
# 1. í•™ìŠµëœ Visual Prompt LoRA ê²½ë¡œ
ADAPTER_DIR = "./checkpoints/mvsm_visual_prompt_v1"

# 2. ë² ì´ìŠ¤ ëª¨ë¸ (ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨)
BASE_MODEL_ID = "Qwen/Qwen2.5-VL-3B-Instruct"

# 3. ì €ì¥í•  ê²½ë¡œ
OUTPUT_DIR = "./checkpoints/mvsm_visual_prompt_merged"
# ========================================

def merge():
    print(f"ğŸš€ Loading Base Model (Offline)...")
    base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        BASE_MODEL_ID,
        torch_dtype=torch.float16,
        device_map="cpu",
        local_files_only=True
    )

    print(f"ğŸš€ Loading LoRA Adapter...")
    model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)
    
    print("ğŸ”„ Merging...")
    model = model.merge_and_unload()
    
    print(f"ğŸ’¾ Saving merged model to: {OUTPUT_DIR}")
    model.save_pretrained(OUTPUT_DIR)
    
    print("ğŸ’¾ Saving processor...")
    try:
        processor = AutoProcessor.from_pretrained(BASE_MODEL_ID, local_files_only=True)
        processor.save_pretrained(OUTPUT_DIR)
    except:
        processor = AutoProcessor.from_pretrained(ADAPTER_DIR, local_files_only=True)
        processor.save_pretrained(OUTPUT_DIR)
    
    print("âœ¨ Merge Complete!")

if __name__ == "__main__":
    merge()