import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from peft import PeftModel
import os

# ================= μ„¤μ • =================
# 1. ν•™μµλ LoRA μ–΄λ‘ν„° κ²½λ΅
ADAPTER_DIR = "./checkpoints/mvsm_aug_flip_v1"

# 2. λ² μ΄μ¤ λ¨λΈ ID
BASE_MODEL_ID = "Qwen/Qwen2.5-VL-3B-Instruct"

# 3. λ³‘ν•©λ λ¨λΈ μ €μ¥ κ²½λ΅
OUTPUT_DIR = "./checkpoints/mvsm_aug_flip_v1_merged"
# ========================================

def merge():
    print(f"π€ Loading Base Model: {BASE_MODEL_ID} (Offline Mode)")
    
    # [μμ •] local_files_only=True μ¶”κ°€ (μΈν„°λ„· μ°¨λ‹¨, λ΅μ»¬ μΊμ‹ μ‚¬μ©)
    base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        BASE_MODEL_ID,
        torch_dtype=torch.float16,
        device_map="cpu",
        local_files_only=True 
    )

    print(f"π€ Loading LoRA Adapter from: {ADAPTER_DIR}")
    model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)
    
    print("π”„ Merging LoRA into Base Model...")
    model = model.merge_and_unload()
    
    print(f"π’Ύ Saving merged model to: {OUTPUT_DIR}")
    model.save_pretrained(OUTPUT_DIR)
    
    print("π’Ύ Saving processor...")
    # [μμ •] Processorλ„ λ΅μ»¬μ—μ„λ§ μ°Ύλ„λ΅ κ°•μ 
    try:
        processor = AutoProcessor.from_pretrained(BASE_MODEL_ID, local_files_only=True)
        processor.save_pretrained(OUTPUT_DIR)
    except Exception as e:
        print(f"β οΈ Processor λ΅λ“ μ¤‘ κ²½κ³ : {e}")
        print("   -> ν•™μµλ μ²΄ν¬ν¬μΈνΈ ν΄λ”μ—μ„ processor νμΌμ„ λ³µμ‚¬ν•΄μµλ‹λ‹¤.")
        # λ§μ•½ λ² μ΄μ¤ λ¨λΈ λ΅λ“ μ‹¤ν¨μ‹, ν•™μµλ ν΄λ”μ—μ„ λ³µμ‚¬ μ‹λ„
        processor = AutoProcessor.from_pretrained(ADAPTER_DIR, local_files_only=True)
        processor.save_pretrained(OUTPUT_DIR)
    
    print("β¨ Merge Complete! μ΄μ  ν‰κ°€(Evaluate) λλ¦¬μ…”λ„ λ©λ‹λ‹¤.")

if __name__ == "__main__":
    merge()