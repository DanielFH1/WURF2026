import os
import sys

# ==========================================
# 1. GPU ì„¤ì • (3ë²ˆ GPU ì‚¬ìš© - ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ë‹¤ë¥¸ ë²ˆí˜¸ë¡œ ë³€ê²½)
# ==========================================
os.environ["CUDA_VISIBLE_DEVICES"] = "3" 

import cv2
import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor

try:
    from qwen_vl_utils import process_vision_info
except ImportError:
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "qwen-vl-utils"])
    from qwen_vl_utils import process_vision_info

# ==========================================
# 2. ëª¨ë¸ ë¡œë“œ
# ==========================================
print("ğŸš€ Loading Model for VSB Test...")
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct", torch_dtype="auto", device_map="auto"
)
processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")

# VSB ìŠ¤íƒ€ì¼ì˜ ë‹µë³€ í›„ë³´êµ° (ìƒëŒ€ì  ìœ„ì¹˜) [cite: 104]
target_words = ["Left", "Right", "Front", "Back"] # VSBì˜ ì£¼ìš” Direction
target_ids = [processor.tokenizer.encode(w, add_special_tokens=False)[0] for w in target_words]
target_ids_tensor = torch.tensor(target_ids).to("cuda")

# ==========================================
# 3. VSB ìŠ¤íƒ€ì¼ ì‹¤í—˜ ì„¤ì •
# ==========================================
VIDEO_PATH = "/nas_data2/seungwoo/dataset/epic_data/EPIC-KITCHENS/P01/videos/P01_01.MP4"

# [VSB Prompt Template ì ìš©] 
# "Where is the {object1} located compared to the {object2} from the camera's perspective?"
# P01_01 ì˜ìƒ ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ 'Sponge'(ìŠ¤í€ì§€)ì™€ 'Tap'(ìˆ˜ë„ê¼­ì§€) ê´€ê³„ë¥¼ ë´…ë‹ˆë‹¤.
# (ì˜ìƒì„ ë³´ì‹œê³  ë¬¼ì²´ ì´ë¦„ì€ ìˆ˜ì •í•´ì£¼ì„¸ìš”!)
object1 = "sponge"
object2 = "tap"
prompt_text = f"Where is the {object1} located compared to the {object2} from the camera's perspective? Answer with one word: Left, Right, Front, or Back."

print(f"ğŸ“ Prompt: {prompt_text}")

# ==========================================
# 4. ë°ì´í„° ìˆ˜ì§‘ (5ë¶„ ë¶„ëŸ‰ë§Œ í…ŒìŠ¤íŠ¸ - 18000 í”„ë ˆì„)
# ==========================================
# ì „ì²´ë¥¼ ë‹¤ ëŒë¦¬ê¸°ë³´ë‹¤, ë¬¼ì²´ ë‘ ê°œê°€ ê°™ì´ ë‚˜ì˜¤ëŠ” êµ¬ê°„ì´ ì¤‘ìš”í•˜ë¯€ë¡œ
# ì•ë¶€ë¶„ 5000 í”„ë ˆì„ ì •ë„ë§Œ ë¹ ë¥´ê²Œ ëŒë ¤ì„œ ê²½í–¥ì„±ì„ ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.
MAX_FRAMES = 5000 
STRIDE = 5

results_log = []
cap = cv2.VideoCapture(VIDEO_PATH)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
fps = cap.get(cv2.CAP_PROP_FPS)

pbar = tqdm(total=min(MAX_FRAMES, total_frames))
frame_idx = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret or frame_idx >= MAX_FRAMES: break
    
    if frame_idx % STRIDE != 0:
        frame_idx += 1
        pbar.update(1) # ì „ì²´ ì§„í–‰ë¥  ìœ„í•´ ì—…ë°ì´íŠ¸
        continue
        
    # ì¶”ë¡ 
    pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    messages = [{"role": "user", "content": [{"type": "image", "image": pil_img}, {"type": "text", "text": prompt_text}]}]
    
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=1, output_scores=True, return_dict_in_generate=True)
        
    logits = outputs.scores[0][0][target_ids_tensor]
    probs = F.softmax(logits, dim=-1)
    
    pred_idx = torch.argmax(probs).item()
    pred_word = target_words[pred_idx]
    confidence = probs[pred_idx].item()
    
    results_log.append({
        "frame": frame_idx,
        "time": frame_idx / fps,
        "pred_word": pred_word,
        "confidence": confidence,
        "probs": probs.cpu().numpy().tolist() # ì „ì²´ í™•ë¥  ë¶„í¬ ì €ì¥
    })
    
    frame_idx += 1
    pbar.update(1)

cap.release()
pbar.close()

# CSV ì €ì¥
df = pd.DataFrame(results_log)
df.to_csv("vsb_video_test_results.csv", index=False)
print("ğŸ’¾ Data saved to 'vsb_video_test_results.csv'")

# ==========================================
# 5. ì‹œê°í™” (VSB Failure Visualization)
# ==========================================
plt.figure(figsize=(15, 6))

# ë°”ì½”ë“œ ìŠ¤íƒ€ì¼ë¡œ ì‹œê°í™” (ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ë°”ë€ŒëŠ”ì§€ í™•ì¸)
# ìƒ‰ìƒ ë§¤í•‘
color_map = {'Left': 'red', 'Right': 'blue', 'Front': 'green', 'Back': 'orange'}
colors = [color_map.get(w, 'gray') for w in df['pred_word']]

# ì‚°ì ë„ë¡œ í‘œí˜„ (ì‹œê°„ì¶• vs ì˜ˆì¸¡ ë‹¨ì–´)
for word in target_words:
    subset = df[df['pred_word'] == word]
    plt.scatter(subset['time'], [word]*len(subset), c=color_map.get(word), label=word, s=10, alpha=0.6)

plt.plot(df['time'], df['pred_word'], c='gray', alpha=0.2, linestyle=':') # ì—°ê²°ì„  (í”ë“¤ë¦¼ ê°•ì¡°)

plt.title(f"VSB Task on Video: '{prompt_text}'\n(Allocentric Stability Analysis)", fontsize=14)
plt.xlabel("Time (seconds)")
plt.ylabel("Predicted Relative Position")
plt.grid(True, linestyle='--', alpha=0.3)

plt.savefig("vsb_failure_visualization.png", dpi=300)
print("âœ… Visualization saved: 'vsb_failure_visualization.png'")