import os
import json
import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import gc # ê°€ë¹„ì§€ ì»¬ë ‰í„°

# ================= ì„¤ì • =================
MODEL_PATH = "./checkpoints/mvsm_visual_cot_merged"
CLEAN_DATA_PATH = "data_train_scene_split/test.json"
BOXED_DATA_PATH = "data_train_scene_split/test_visual_prompt.json"
IMAGE_ROOT = "/nas_data2/seungwoo/2/ViewSpatial-Bench"
OUTPUT_DIR = "result/saliency_maps_gradient"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# =======================================

def get_actual_image_path(img_path_entry):
    if isinstance(img_path_entry, list): path_str = img_path_entry[0]
    else: path_str = img_path_entry
    full_path_a = os.path.join(IMAGE_ROOT, path_str)
    if os.path.exists(full_path_a): return full_path_a
    if os.path.exists(path_str): return path_str
    parts = path_str.split(os.sep)
    if len(parts) > 1:
        shorter_path = os.path.join(*parts[1:])
        full_path_c = os.path.join(IMAGE_ROOT, shorter_path)
        if os.path.exists(full_path_c): return full_path_c
    return None

def get_saliency_map(model, processor, image_path_entry, text_prompt):
    # ë©”ëª¨ë¦¬ ì²­ì†Œ
    torch.cuda.empty_cache()
    gc.collect()

    full_path = get_actual_image_path(image_path_entry)
    if not full_path:
        print(f"âŒ Image missing: {image_path_entry}")
        return None, None
    
    image = Image.open(full_path).convert("RGB")
    
    messages = [{"role": "user", "content": [{"type": "image", "image": full_path}, {"type": "text", "text": text_prompt}]}]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    
    inputs = processor(
        text=[text], 
        images=image_inputs, 
        videos=video_inputs, 
        padding=True, 
        return_tensors="pt"
    ).to(model.device)
    
    # í”½ì…€ì— ëŒ€í•œ Gradient ì¶”ì  ì„¤ì •
    if 'pixel_values' in inputs:
        inputs['pixel_values'].requires_grad_(True)
        inputs['pixel_values'].retain_grad()
    else:
        return None, None

    # Forward Pass
    model.zero_grad()
    
    # [í•µì‹¬] Gradient Checkpointingì´ ì¼œì ¸ ìˆìœ¼ë©´ forward ì‹œ ë©”ëª¨ë¦¬ë¥¼ ì•„ë‚Œ
    outputs = model(**inputs)
    
    logits = outputs.logits
    # ê°€ì¥ ë§ˆì§€ë§‰ì— ìƒì„±ë  í† í°ì˜ Logitì„ íƒ€ê²Ÿìœ¼ë¡œ ì¡ìŒ
    # (Qwen2-VLì€ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ì§ì „ì˜ ìƒíƒœ)
    next_token_logits = logits[0, -1, :]
    target_token_index = next_token_logits.argmax()
    score = next_token_logits[target_token_index]
    
    # Backward Pass
    score.backward()
    
    gradients = inputs['pixel_values'].grad
    if gradients is None:
        print("âŒ Gradients are None.")
        return None, None

    # Saliency ê³„ì‚° (ì±„ë„ í‰ê· )
    saliency = gradients.abs().mean(dim=-1).detach().cpu() # CPUë¡œ ë°”ë¡œ ë‚´ë¦¼
    
    # ë©”ëª¨ë¦¬ í•´ì œ
    del gradients, outputs, logits, score
    torch.cuda.empty_cache()

    # Grid ë³µì›
    grid_thw = inputs['image_grid_thw'][0]
    h, w = grid_thw[1], grid_thw[2]
    expected_len = h * w
    
    # Qwen2-VL 2x2 Pooling ê³ ë ¤ (visual tokens = h//2 * w//2)
    # í•˜ì§€ë§Œ pixel_valuesì˜ ê¸¸ì´ëŠ” ë³´í†µ h*w (Before pooling) ì´ê±°ë‚˜ pooling í›„ì¼ ìˆ˜ ìˆìŒ.
    # pixel_values.gradì˜ shape[0] í™•ì¸ í•„ìš”.
    # pixel_values shapeì€ [Total_Pixels, Channels] (Flattened patches)
    
    # ë§Œì•½ saliency ê¸¸ì´ê°€ h*wì™€ ê°™ë‹¤ë©´:
    if saliency.shape[0] == expected_len:
        heatmap = saliency.view(h, w).float().numpy()
    else:
        # ê¸¸ì´ê°€ ì•ˆ ë§ìœ¼ë©´ (ë³´í†µ Pooling ë•Œë¬¸)
        # Vision Tokens (h//2 * w//2) ë§Œí¼ë§Œ ë’¤ì—ì„œ ìë¦„
        vision_len = (h//2) * (w//2)
        if saliency.shape[0] >= vision_len:
            saliency = saliency[-vision_len:]
            heatmap = saliency.view(h//2, w//2).float().numpy()
        else:
            print(f"âš ï¸ Shape Mismatch: {saliency.shape} vs {h}x{w}")
            return None, None

    # ë¦¬ì‚¬ì´ì§• ë° ì •ê·œí™”
    img_w, img_h = image.size
    heatmap = cv2.resize(heatmap, (img_w, img_h))
    
    # ë…¸ì´ì¦ˆ ì œê±° (ìƒìœ„ 1% í´ë¦¬í•‘)
    threshold = np.percentile(heatmap, 99)
    heatmap = np.clip(heatmap, 0, threshold)
    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)
    
    return image, heatmap

def visualize_comparison(model, processor, clean_item, boxed_item, idx):
    print(f"\nğŸ¨ Processing ID {idx}...")
    
    try:
        img_clean, map_clean = get_saliency_map(model, processor, clean_item['image_path'], clean_item['question'])
        if map_clean is None: 
            print("   âŒ Clean map generation failed.")
            return

        img_boxed, map_boxed = get_saliency_map(model, processor, boxed_item['image_path'], boxed_item['question'])
        if map_boxed is None: 
            print("   âŒ Boxed map generation failed.")
            return

        # ì‹œê°í™” ì €ì¥
        fig, axes = plt.subplots(1, 4, figsize=(20, 5))
        
        axes[0].imshow(img_clean)
        axes[0].set_title("Original Image")
        axes[0].axis('off')
        
        axes[1].imshow(img_clean)
        axes[1].imshow(map_clean, alpha=0.6, cmap='jet')
        axes[1].set_title("Original Saliency")
        axes[1].axis('off')
        
        axes[2].imshow(img_boxed)
        axes[2].set_title("Visual Prompt Image")
        axes[2].axis('off')
        
        axes[3].imshow(img_boxed)
        axes[3].imshow(map_boxed, alpha=0.6, cmap='jet')
        axes[3].set_title("Boxed Saliency\n(Tunnel Vision Check)")
        axes[3].axis('off')
        
        save_path = os.path.join(OUTPUT_DIR, f"saliency_map_{idx}.png")
        plt.savefig(save_path, bbox_inches='tight')
        plt.close(fig) # ë©”ëª¨ë¦¬ í•´ì œ
        print(f"   âœ… Saved: {save_path}")
        
    except Exception as e:
        print(f"   âŒ Error processing ID {idx}: {e}")
        import traceback
        traceback.print_exc()

def main():
    print("ğŸš€ Initializing...")
    
    # [ìµœì í™” 1] bfloat16 ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆë°˜)
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        MODEL_PATH, 
        torch_dtype=torch.bfloat16, 
        device_map="cuda:0",
    )
    
    # [ìµœì í™” 2] Gradient Checkpointing í™œì„±í™” (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ)
    model.gradient_checkpointing_enable()
    
    # Checkpointing ì‚¬ìš© ì‹œ ì…ë ¥ì˜ Gradientsë¥¼ ì¼œì¤˜ì•¼ í•¨
    model.enable_input_require_grads()
    
    processor = AutoProcessor.from_pretrained(MODEL_PATH)
    
    print("âœ… Model loaded with optimizations (bf16 + checkpointing)")

    # ë°ì´í„° ë¡œë“œ
    with open(CLEAN_DATA_PATH, 'r') as f: clean_data = json.load(f)
    with open(BOXED_DATA_PATH, 'r') as f: boxed_data = json.load(f)
    
    clean_map = {i: item for i, item in enumerate(clean_data)}
    boxed_map = {i: item for i, item in enumerate(boxed_data)}
    
    target_ids = [i for i, item in enumerate(clean_data) if "Scene Simulation" in item.get('question_type', '')]
    target_ids = target_ids[:5] # 5ê°œë§Œ í…ŒìŠ¤íŠ¸
    
    print(f"ğŸ§ª Generating Maps for {len(target_ids)} samples...")
    
    for idx in target_ids:
        if idx in clean_map and idx in boxed_map:
            visualize_comparison(model, processor, clean_map[idx], boxed_map[idx], idx)

    print("âœ… All Done!")

if __name__ == "__main__":
    main()